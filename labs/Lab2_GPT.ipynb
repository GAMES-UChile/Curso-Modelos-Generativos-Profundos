{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3ec1cec624ef406c8f6a8d3cb7b93d46",
    "deepnote_cell_type": "markdown",
    "id": "e4Pr6pv5ie50"
   },
   "source": [
    "**MDS7203 Modelos Generativos Profundos, Primavera 2023**\n",
    "\n",
    "# Laboratorio 2: Modelo de lenguaje auto-regresivo\n",
    "\n",
    "**Profesor**: Felipe Tobar, **Auxiliares**: Cristóbal Alcázar, Camilo Carvajal Reyes, **Ayudante**: Joaquín Barceló.\n",
    "\n",
    "**Fecha de entrega**: viernes 29 de septiembre 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38cd31c023b047f080018f94f24927e8",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "**Nombre: COLOCAR AQUÍ SU NOMBRE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "279c16d9f94a4b5dbe45f9f4f6ff6f07",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "**Instrucciones**: El presente notebook contiene enunciado e instrucciones para la realización del laboratorio. Usted deberá completar los códigos (en este archivo o en una copia del mismo) donde se le pida hacerlo. Usted deberá entregar el notebook con sus respuestas, cumpliendo lo siguiente:\n",
    "\n",
    "- Los comentarios en código deben ser concisos pero claros. No se evaluarán sub-preguntas donde solo exista código sin comentarios pertinentes.\n",
    "- El código debe ser ordenado y ejectuable. No se evaluarán notebooks o scripts que generen errores en su ejecución. Se aconseja resetear la kernel y corroborar la correcta execución de todas las celdas antes de ejecutar el entrenamiento de su modelo.\n",
    "- Si bien se aconseja el uso de internet y otras herramientas para asistir su trabajo, asi como discusiones con el ED y estudiantes, el código que entregue debe ser de su autoría.\n",
    "\n",
    "El objetivo del laboratorio será implementar, desde casi cero, un modelo de lenguaje estilo GPT, i.e., basado en el uso de un bloque \"decoder\" de la arquitectura Transformer (como se muestra en la imagen a continuación). Este tipo de modelos es un ejemplo de modelo auto-regresivo y que ha tenido gran relevancia en el último tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://i.stack.imgur.com/bWnx0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos links útiles:\n",
    "\n",
    "* [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
    "* [GPT2, original blog post](https://openai.com/research/better-language-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "40d3aa47f3204ca888f874beb273d9f5",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Resumen de preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "14369c673f3049a19e647a78a00686d4",
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] a) (0,5 ptos.) Definición de diccionarios para vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3ae5920aed6440ea4c012068c08bb8b",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] b) (bonus) Utilización de embeddings previo a la normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d259ba9b76b74b2e8eb0132b06f7f31c",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] c) (bonus) Escalamiento por $1/\\sqrt{d_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "14547d4ff1564ceca8f1133b2bf3831f",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] d) (1.5 ptos.) Creación de clase `Head`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "806e04b64a5f4e8c9c46b8addb61da30",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] e) (0.75 pto.) Implementación de clase `FeedForward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "af2c2194e9e94407b2ed8d38ac7dd9c5",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] f) (0.5 ptos.) Relación entre hiper-parámetros n_head y head_size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9cec4a573e824ff2a535781db9773328",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] g) (0.75 ptos.) Forward pass en `DecoderBlock`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "33851255223c4c569a4ef98add7b0b4f",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] h) (1 pto.) Implementación clase `GPTLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "273f32755bf446cba8132463ead5c3ac",
    "checked": false,
    "deepnote_cell_type": "text-cell-todo",
    "formattedRanges": []
   },
   "source": [
    "- [ ] i) (1 pto.) Training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] j) (bonus) Comparar modelo con Baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3381cfa7678849408eeaf9aedb7fb2d1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5134,
    "execution_start": 1693917871424,
    "id": "xIPkmAHJmC9s",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características de la GPU, si es que está disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2bd1045c4e41414b88df0bae08b56b58",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 474,
    "execution_start": 1693859116039,
    "id": "CGoaBP7mygL4",
    "outputId": "921a9201-f250-4b10-a82a-280ac6ed6a03",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a607d5c593c64504a2c445b78a01b637",
    "deepnote_cell_type": "markdown",
    "id": "O54vJC-DjbMe"
   },
   "source": [
    "## 1. Corpus 📖\n",
    "\n",
    "Escoja uno de los dos datasets:\n",
    "- shakespeare.txt: concatenación de obras de shakespeare, ~ 1 millon de caracteres\n",
    "- cabromagico.txt: concatenación de los libros de Harry Potter, ~ 6 millones de caracteres\n",
    "\n",
    "Escoja en base a sus gustos y capacidades de cómputo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si usan collab, descargar dataset desde repo GAMES descomentando una de las siguientes lineas:\n",
    "# !wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/shakespeare.txt\n",
    "# !wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/cabromagico.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "23f881bed52248f8bfd4204bbdf4f3d1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1693864731107,
    "id": "oC8C6LkJi21K",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# with open('cabromagico.txt', 'r', encoding='utf-8') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "print(f\"Tamaño del corpus: {len(text):,} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "10a13a6c0fd849b584a6ab3a4f5991e6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1693859154739,
    "id": "Z-IuIqKHj0zL",
    "outputId": "fa8f8d5c-d2cb-463f-f0aa-37ddf0eddc79",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "063a10ca987f4b479fb3b53b92f2ad52",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> a) (0.5 ptos.) Dada una lista de ordenada de caracteres, defina:\n",
    "> - stoi: un diccionario caracter -> índice\n",
    "> - itos: un diccionario índice -> caracter\n",
    "> Con lo anterior, defina dos funciones encode y decode que tomen un string y una lista de índices respectivamente y devuelvan una lista de índices y un string según corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "599ecea77d984abaace18d2f1cc934f1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1693859157194,
    "id": "NURlEkGjkSEf",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "\n",
    "stoi = pass\n",
    "itos = pass\n",
    "\n",
    "# encoder: toma un string, devuelve una lista de índices\n",
    "encode = lambda s: None  # CAMBIAR\n",
    "\n",
    "# decoder: toma una lista de índices, devuelve un string\n",
    "decode = lambda l: None  # CAMBIAR\n",
    "\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e29011090a174795a80acd43b9e368ee",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "assert encode('hola, que tal?') == [46, 53, 50, 39, 6, 1, 55, 59, 43, 1, 58, 39, 50, 12], 'Verifica que el output entregue una lista de enteros'\n",
    "assert decode(encode('hola, que tal?')) == 'hola, que tal?', 'Debe ser un string'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "488abcf1ec984a438cab8626fc6e8160",
    "deepnote_cell_type": "markdown",
    "id": "ktvBOC1qmJDY"
   },
   "source": [
    "Nuestro modelo no entiende el lenguaje directamente, sino que los representa como números. Pasamos el corpus completo a su representación de enteros, usando el `stoi` (aka _string-to-index_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9e79dd31677a477580450a4b80e2aa95",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 136,
    "execution_start": 1693859161854,
    "id": "WAsyhejrl2X6",
    "outputId": "00a2f69e-3a81-4e2f-a0f1-89a65b6a9efb",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "39c418201e2a4534a13ddc5527929041",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "d9aOmlLVlEcr",
    "outputId": "837c4a39-03c9-4070-8f9a-495ae235dce7"
   },
   "outputs": [],
   "source": [
    "N=100\n",
    "print(f\"Texto con los primeros {N} caracteres:\\n----------------------------------------------\\n\")\n",
    "print(text[:N])\n",
    "print(\"\\n----------------------------------------------\\nSu representación como tensor de PyTorch...\\n\")\n",
    "print(data[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5aa3280c539a40c7a350543d116126c6",
    "deepnote_cell_type": "markdown",
    "id": "Aqdw5-8BnVhX"
   },
   "source": [
    "## 2. Separar el dataset 🔨 y 🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4ee3ee4d9f4c425e86da95a874ce263f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1693859215823,
    "id": "zzUjG0OqnWr4",
    "outputId": "83ca4977-0452-4329-c2e5-a7c40449faf1",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # 90%\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"--> Tamaño del corpus de entrenamiento: {len(train_data):,} ({(train_data.shape[0] / data.shape[0]):.2f}) caracteres\")\n",
    "print(f\"--> Tamaño del corpus de validación: {len(val_data):,} ({(val_data.shape[0] / data.shape[0]):.2f}) caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91dfd4134b1c44ba8b699996afd1cc7d",
    "deepnote_cell_type": "markdown",
    "id": "JaWNJQnzpzC8"
   },
   "source": [
    "Sobre la mécanica de datos y etiquetas,\n",
    "\n",
    "* Accedemos a los datos a partir de \"fragmentos contextuales\"; esto es un bloque de texto en representación númerica de tamaño `block_size`\n",
    "* El modelo es semi-supervisado, es decir, búscamos entrenar un modelo de tal forma que dado ${x}_{i:j}$ _tokens_, vamos a predecir el siguiente _token_ $x_{j+1}$\n",
    "* Las etiquetas emergen del mismo bloque contextual moviendo la ventana con un _offset_ de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "97a1b7a307504727b77d318f9923413a",
    "deepnote_cell_type": "markdown",
    "id": "zFVaZzxuq0H3"
   },
   "source": [
    "Por ejemplo, dado un bloque de tamaño 8,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b4b1cc1b8e7d4b9795c52191988b8056",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 23,
    "execution_start": 1693859220379,
    "id": "ouHuc50ApdRS",
    "outputId": "212050e2-1b1b-42e1-8a9f-931c0976287c",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "block_size = 13\n",
    "print(f\"Una bloque contextual (X, Y) será:\\n\")\n",
    "print(f\"X: {[x.item() for x in data[:block_size]]}\")\n",
    "print(f\"  --> decode(X): {decode([x.item() for x in data[:block_size]])}\")\n",
    "print('------------------------------------')\n",
    "print(f\"Y: {[x.item() for x in data[1:block_size+1]]}\")\n",
    "print(f\"  --> decode(Y): {decode([y.item() for y in data[1:block_size+1]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d2f37b21b42246a3adfb3998f07f9387",
    "deepnote_cell_type": "markdown",
    "id": "ukclPGdHwALB"
   },
   "source": [
    "Sin embago, dentro de cada bloque contextual ocupamos la información de manera autoregresiva, generando múltiple observaciones a partir de este..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "07f3c398835145b38ffa4db9f8b924e6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1693859223706,
    "id": "NBir0GCgshQH",
    "outputId": "46ed83b4-aa7a-40d7-e70e-b3690645686b",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Cuando el input es {context} el target es: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7ed1dfd8af584e109c9348cd1d0912db",
    "deepnote_cell_type": "markdown",
    "id": "AeRBbWV6z75E"
   },
   "source": [
    "Por lo tanto, cada bloque contextual, genera un número de observaciones igual a su tamaño.\n",
    "\n",
    "En términos de _batches_, podemos procesar en paralelo, múltiples bloques contextuales. Lo importante es que cada bloque contextual es independiente, y no hay computo que ocurra a nivel transversal, sino paralelo entre estos. No se mezclan las secuencias autoregresivas de cada contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "779633ce16194ccfbcc43f79bf338b7a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 265,
    "execution_start": 1693859227529,
    "id": "X3hWnI_5yp7x",
    "outputId": "a01f72b2-2644-4065-f149-e622a55790bc",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# colocar seed como su RUT\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8  # largo de ventana máximo para considerar en la precisión\n",
    "# Estos parámetros se re-definirán para el entrenamiento final\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"Cuando el input es {context.tolist()} el target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1889b0faf3d648409045388624948ef6",
    "deepnote_cell_type": "markdown",
    "id": "U-xAeNbk1f2E"
   },
   "source": [
    "Lo que recibirá la red como _input_ será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ace2e494f0af47718468b6e75eb1dd54",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1693859236814,
    "id": "gTCL2aQY1VbI",
    "outputId": "a1b07199-d63c-4f5c-e874-86af4f43a774",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8f4d24230cbc46af97f97d7090c150d4",
    "deepnote_cell_type": "markdown",
    "id": "OPG_QkSJ1km7"
   },
   "source": [
    "## 3. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e48f01409934cf88739d426f3a1b144",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Creamos un modelo base clásico, para luego compararlo con nuestro Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "79776665f2914e0dad0b035148df8a82",
    "deepnote_cell_type": "code",
    "id": "MlAT7Lnx1hyd"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # cada token lee sucesivamente los logits para el token siguiente de una lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx y targets son ambos tensores de tamaño (B,T) con elementos enteros\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx es un arreglo (B, T) de indices del contexto actual\n",
    "        for _ in range(max_new_tokens):\n",
    "            # obtener predicciones\n",
    "            logits, loss = self(idx)\n",
    "            # concentrarse en el último paso\n",
    "            logits = logits[:, -1, :]  # se convierte en (B, C)\n",
    "            # aplicamos softmax para obtener probabilidades\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # samplear de la distribución\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # agregar el índice de la muestra a la secuencia actual\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d163007f6061430fa982a27a84438b23",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## 4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0f104b88006e4923a61a73a4d2c7cf87",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "En la sección anterior, vimos que cada _token_ toma una representación vectorial llamada _embeddings_.\n",
    "\n",
    "Nuestro corpus contiene $65$ caracteres únicos con un _embedding_ asociado a cada _token_. La idea de representar el lenguaje en términos de tokens, y estos a su vez en vectores, es que podemos aprender estas representaciones vectoriales a partir de los datos. Sin embargo, la representación es única, y muchas veces un mismo _token_ puede tener distintos significados según su contexto. Por ejemplo:\n",
    "1. \"Te banco a morir!\"\n",
    "2. \"El banco está abierto hasta las dos.\"\n",
    "\n",
    "En ambas oraciones anteriores, el token `banco` tiene un significado distinto. Se espera entonces que la representación de ese token sea distinta en ambos casos y eso lo logramos con la influencia de los tokens presentes en el mismo contexto.\n",
    "\n",
    "La idea principal de _self-attention_ es utilizar la secuencia de _embeddings_ dentro de un contexto para computar un promedio ponderado a partir de estos. Dado una secuencia de _embeddings_ de _tokens_ $x_1, \\dots, x_n$, el mecanismo de _self-attention_ (o auto-atención) produce una nueva secuencia de _embeddings_ $x'_1, \\dots, x'_n$, donde cada $x'_i$ es una combinación lineal de todos los $x_j$:\n",
    "\n",
    "$$\n",
    "x'_i = \\sum_{j=1}^{n} \\alpha_{ij} x_{j}\n",
    "$$\n",
    "\n",
    "Los coeficientes $\\alpha_{ij}$ se llaman ponderadores de atención y están normalizados tal que $\\sum_{j}\\alpha_{ji}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9e921ba45c064d5d9628b6dc9f470c9b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "En términos sencillos, construiremos un mecanismo de comunicación entre distintos tokens dentro del bloque de contexto, que se representará por una colección de ponderadores en una matriz. Esta colección de ponderadores la llamaremos matriz de atención (o self-attention) y nos permitirá vía la operación de multiplicación de matrices, agregar distintos valores dentro de un bloque contextual en una sola cantidad. Spoiler, estos pesos serán data-dependientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9296b572acca4bad82f5cfc62690af5b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Comencemos emulando la operación con pesos fijos, usaremos la parte triangular inferior de una matriz identidad de 3x3, la cual normalizaremos a nivel de fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b8c36015c3a24412a5f725c06a0e08ac",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1693860540618,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Ejemplo de juguete que ilustra como la multiplicación matricial puede ser usada para una adición con pesos\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=',a,'\\n')\n",
    "print('b=',b,'\\n')\n",
    "print('c=',c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff4d0f0d742e4f9ba3bcb7f6e8e560ef",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Notemos que $c$ tiene en cada fila los resultados de los valores acumulados de $b$ según los ponderadores de $a$.\n",
    "\n",
    "El tensor $a$ se interpreta como una matriz de token-a-token y representa la interación/influencia del token en la posición $i$ con el token de la posición $j$. Dado que nuestro modelo es autoregresivo, los tokens del presente solo pueden ser influenciados por tokens pasados, o ellos mismos. Por eso las posiciones de $a$ que cumplen esta restricción $i \\leq j$, son elementos que conforman la matriz triangular inferior de $a$. El resto de las posiciones no tiene influencia sobre los tokens pasados (i.e. 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f3c3eaff6931470ab66a89baf7c2b884",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Vamos a crear un _batch_ con datos síntetico de tamaño `B`, donde cada bloque contextual será de largo $T$, y cada _token_ que compone el contexto se representa por $C$ dimensiones (i.e. tamaño del _embedding_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e2e10fa5f831461daec2ca0c93b27a04",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1693861772506,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2aa3b323c83c4ac4a4f7ea1930ca253c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1693862258062,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Versión usando softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f89fedd9c5664d448a0f73d90c8c53ca",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1693862260341,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f647ef2e2b1d4f589593fb378fe2616f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Los ponderadores anteriores son uniformes, ahora introducimos los conceptos de _queries_ y _keys_ para ver como computar los ponderadores a partir de los datos.\n",
    "\n",
    "* Un _query_ $q(\\cdot)$ corresponde a una proyección lineal de la representación de _embeddings_ de un token particular. Por ejemplo, se proyecto $\\mathbb{R}^{C}\\rightarrow \\mathbb{R}^{H}$.\n",
    "* Los _keys_ es la matriz $K\\in\\mathbb{R}^{T\\times H}$ que contiene proyecciones lineales de todos los _embeddings_ de tokens dentro del contexto, incluído el token que es el query. La proyección lineal de los _keys_ es de igual tamaño (i.e. $H$) que la proyección del _query_.\n",
    "* Los ponderadores para cada _query_ se obtiene a partir de qué tan próximo se relaciona un token respecto al resto de los token dentro de un contexto. Por ejemplo, $q(x_i) \\times K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "a4f9611bdc5f426b87b42952cdf55d51",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2bdc17d026094321b7b1b6b53fd3bec9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> b) (Bonus): Explique porqué no se utilizan directamente los _embeddings_ para computar la matriz de atención previo a la normalización, i.e. `(T,B).dot((B,T))`, en vez de usar las proyecciones $QK^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8bdb8dd7d9954b87b248303181315e57",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> c) (Bonus): Explique los argumentos detras de escalar por $1/\\sqrt{d_k}$ referidos en el paper _[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_ (Vaswani 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0a0761b6973342598468b3f265e85223",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1693864851430,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# colocar seed como su RUT\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32  # batch, time, channels\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff534d33b5ae46b5a252eaa2023dfa39",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Ejemplo de aplicación de módulo de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "64725c524c18451a843b2f837bd65dcd",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5db2feb142554dddb5ce05738c1f743e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Observaciones:\n",
    "- Atención es un **mecanismo de comunicación**. Puede ser entendido como nodos en un grafo dirigido conectándose unos con otros y agregando información con una suma ponderada de todos los nodos que apuntan a ellos, con pesos dependientes de los datos.\n",
    "- No hay una noción de espacio. Atención simplemente actua sobre el conjunto de vectores. Es por este motivo que se necesitan encoders posicionales.\n",
    "- Cada punto dentro de un batch es, desde luego, procesado de manera independiente y nunca intractua con los otros.\n",
    "- En un bloque de atención \"encoder\" basta comentar la linea que hace masking con `tril`, que hace que los tokens se comuniquen todos con todos. El bloque anterior se llama \"decoder\" porque aplica un masking triangular y se encuentre frecuentemente en configuraciones autoregresivas.\n",
    "- \"auto-atención\" (_self-attention_) sólo signfica que tanto _keys_ como _values_ son producidas desde la misma fuente que las _queries_. En \"atención-cruzada\" (_cross-attention_), las _queries_ vienen de $x$, pero _keys_ y _values_ vienen de otra fuente externa (como puede ser un modulo encoder).\n",
    "- Atención \"escalada\" divide `wei` por $\\frac{1}{\\sqrt{head\\_size}}$. Esto hace que cuando los input $Q$ y $K$ tengan varianza unitaria, `wei` también tendrá varianza unitaria y evitará la saturación de la Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "833c93c83a6f42e6a9c6e3adce82a4be",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> d) (1.5 ptos.) Cree una clase `Head` que implemente un módulo de auto-atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e86d22f778c4464894ee70eeb6f47ae1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "n_embd = 64  # dimensionalidad del input\n",
    "dropout = 0.0\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Una cabeza de auto-atención \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # ------------------------\n",
    "\n",
    "        self.key = pass  # Colocar matriz que computa estos valores desde un input\n",
    "        # E igualmente para value y query\n",
    "\n",
    "        # ------------------------\n",
    "        # HINT: cuando aplique tril, ocupe self.tril se define automaticamente\n",
    "        # al instanciar\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input size (batch, time-step, channels)\n",
    "        # output size (batch, time-step, channels)\n",
    "        B,T,C = x.shape\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # Aplicar las funciones anteriores\n",
    "        k = pass  # (B,T,C)\n",
    "        q = pass  # (B,T,C)\n",
    "        v = pass  # (B,T,C)\n",
    "\n",
    "        # Computar los score de atención (\"affinities\")\n",
    "        wei = pass  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = pass  # (B, T, T)\n",
    "        wei = pass  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Adición (con pesos) de las atenciones\n",
    "        out = pass  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "    \n",
    "        # --------------------------------------\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96c2b9cefd6b453cb437392b7f03f5d4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "La arquitectura decoder del paper Transformer implementa varias versiones de _self-attention_ en paralelo, cada una es una \"cábeza de atención\", y estas concatenan sus resultados en un modulo conocido como `MultiHeadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "214c8ece545f4817b8561dfae0db408a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Múltiples cabezas de auto-atención en paralelo \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5d5e70b87ba948e6b2db556e91caeac7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> e) (0.75 pto.) Implemente una clase `FeedForward` como se describe en el artículo [\"Attention is all you need, Vaswani et al.\"](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2bd2438a5180464c873da561e6a8527c",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" \n",
    "        Implementar FeedForward descrita en sección:\n",
    "         \"3.3 Position-wise Feed-Fordward Networks\", paper\n",
    "         \"Attention is All You Need\"\n",
    "        https://arxiv.org/pdf/1706.03762.pdf\n",
    "        \n",
    "        in: n_embd\n",
    "        out: n_embd\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # -------------------\n",
    "\n",
    "            # Defina las operaciones a razlizar, secuencialmente al input\n",
    "            # Recuerde que la primera capa aumenta el tamaño del input en un facor de 4\n",
    "            # Luego en la última volvemos a la dimensionalidad inicial\n",
    "\n",
    "            # ------------------\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "fd893f1e2d6842b08beb90dd1091e678",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e0384b1c93274e7abfb20d5627b2958c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> f) (0.5 ptos.) Explique la relación entre los hiperparámetros `n_head` y `head_size` de la clase `MultiHeadAttention`. Piense en su rol dentro del bloque decoder (i.e. atención + feedforward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a armar la clase que implemente un bloque de Decoder. Nótese que en realidad estamos codificando texto con este bloque, pero esto corresponde a la parte \"Decoder\" del Transformer original, por ende guardamos esa nomenclatura. La motivación del uso del decoder es modelar el texto de maner auto-regresiva, que es ideal para la generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.stack.imgur.com/bWnx0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "708d555c66ac4f4a940e82ab2251cb76",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> g) (0.75 ptos.) Complete el paso _forward_ de la clase `DecoderBlock`. Recuerde en particula incorporar las conexiones residuales (_skip connections_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8a013f2aceba4d7eb70390a12f21c308",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"  BloqueTransformer: COMUNICACIÓN seguida de CÓMPUTO \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: dimensión de embeddings, n_head: número de cabezas de atención\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hint: aplique las capas de normalización siempre antes de otra capa (Pre-LN varaint)\n",
    "        # Ver: https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure\n",
    "        # ---------------------\n",
    "\n",
    "        x = pass  # completar\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cd75ed98e96247ffa253556488b53e4e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 5. Modelo GPT: Juntando todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00e10b6c07f5442b95e5c2b0819edc0a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> h) (1 pto.) Complete el código de la clase `GPTLanguageModel`procesando adecuadamente el input del modelo. Complete además el método `generate` para samplear elementos que completen auto-regresivamente una secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "88ed14ab007b4feea097aaa558f1cb73",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # cada token lee directamente los logits para el token siguiente de una lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx y targets son ambos tensores (B,T) de enteros\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        # -----------------------------\n",
    "\n",
    "        x = pass  # DEFINIR x, con shape (B,T,C)\n",
    "\n",
    "        # procesar x...\n",
    "\n",
    "        logits = pass  # CALCULAR logits (B,T,vocab_size)\n",
    "\n",
    "        # -------------------------------\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx es un arreglo (B, T) de índices en el contexto actual\n",
    "        for _ in range(max_new_tokens):\n",
    "            # restringir idx a los últimos block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # obtener las predicciones\n",
    "            logits, loss = self(idx_cond)\n",
    "            # enfocarse sólo en el último paso\n",
    "            logits = logits[:, -1, :]  # se convierte en size (B, C)\n",
    "            # ------------------------------------------------------------\n",
    "\n",
    "            # aplicar softmax para obtener las probabilidades\n",
    "            probs = pass  # tensor de dimensionalidad (B, C)\n",
    "\n",
    "            # definir una distribución apropiada para samplear usando las probabilidades\n",
    "            idx_next = pass  # tensor de índices de dim (B, 1)\n",
    "\n",
    "            # adjuntar el token generado a la secuencia (actualizando idx)\n",
    "            idx = pass  # tensor resultante de dimensionalidad (B, T+1)\n",
    "            \n",
    "            # ------------------------------------------------------------\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "56fa2d0e8f424060841dfb5a0793b8e8",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Definimos hiperparámetros (global variables, esto se puede hacer mucho mejor)\n",
    "\n",
    "batch_size = 16 # cuantos secuencias de fragmentos del corpus procesaremos de manera independiente (aka B)?\n",
    "block_size = 128 # cuál será el tamaño del bloque de contexto a considerar para predecir (aka T)?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4  # modelo de tamaño pequeño probar esta lr por defecto\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384  # se debe considerar en conjunto con n_head, según análisis en (f)\n",
    "n_head = 6    # se debe considerar en conjunto con n_embd, según análisis en (f)\n",
    "n_layer = 6  \n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "model.to(device)\n",
    "\n",
    "# printear el número de parámetros del modelo\n",
    "print('Número de parámetros del modelo:', sum(p.numel() for p in model.parameters())/1e6, 'millones')\n",
    "\n",
    "# definir el optimizador de PyTorch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos una función para generar batches de secuencias a partir de nuestro corpus de entrenamiento o validación. Además de una función para obtener estimaciones de la función de costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos un \"DataLoader\"\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# definimos una función para obtener estimados de nuestras\n",
    "# pérdidas tanto en los conjuntos de train como en val\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer sanity check, obtener batches con dimensiones correctas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifiquemos que las dimensiones sean correctas\n",
    "xb, yb = get_batch('train')\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo sanity check, verificar que la data fluya correctamente por el modelo,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifiquemos que el forwardpass del modelo no tenga problemas\n",
    "model(xb)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6cd551bb57f54567a9780b868c2081e7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> i) (1 pto.) Complete el bucle de entrenamiento usando comandos de `pytorch.optimize` conocidos. Ejecute el entrenamiento (se recomienda dejarlo corriendo e ir hacer algo más...). Corrobore que se modelo es capaz de generar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "faa0a3b9283247f1a8cf6d6b8d599959",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# colocar RUT como semilla\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# comenzamos el training loop...\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # de vez en cuando evaluar la loss en los conjuntos de train y evaluación\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # samplear un batch de datos\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # activar el optimizador y hacer el paso backward con el resultado de la función loss\n",
    "\n",
    "    # ----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5a8d031142ab45d48e9b9eeb81f6b2f7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 6. Generando secuencias de texto con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9e6d66e2a3ba49a4ae3c23d4e65d57df",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Generar usando el modelo\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "# Para escribir en un archivo\n",
    "# open('output.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> j) (bonus) Define un training loop para el baseline (modelo bi-grama). Entrénelo usando un npumero similar de épocas y compare las losses y generación de texto con su modelo anterior."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "409cf1afb7654c8282dccc46f347e63f",
  "deepnote_persisted_session": {
   "createdAt": "2023-09-04T22:31:43.624Z"
  },
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "aml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
